{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jQ7tQ3xKpBQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tonAZLS4K5tP"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.Conv1 = nn.Conv2d(1, 64, 3, stride=1, padding=1)\n",
        "        self.BN1 = nn.BatchNorm2d(64)\n",
        "        self.MaxPool = nn.MaxPool2d(2, 2)\n",
        "        self.Conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.BN2 = nn.BatchNorm2d(128)\n",
        "        self.Conv3 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.BN3 = nn.BatchNorm2d(256)\n",
        "        self.Conv4 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.BN4 = nn.BatchNorm2d(256)\n",
        "        self.Conv5 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.BN5 = nn.BatchNorm2d(512)\n",
        "        self.Conv6 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.BN6 = nn.BatchNorm2d(512)\n",
        "        self.Conv7 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.BN7 = nn.BatchNorm2d(512)\n",
        "        self.Conv8 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.BN8 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.FC1 = nn.Linear(512, 4096)\n",
        "        self.FC2 = nn.Linear(4096, 4096)\n",
        "        self.FC3 = nn.Linear(4096, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.MaxPool(functional.relu(self.BN1(self.Conv1(x))))\n",
        "        output = self.MaxPool(functional.relu(self.BN2(self.Conv2(output))))\n",
        "        output = functional.relu(self.BN3(self.Conv3(output)))\n",
        "        output = self.MaxPool(functional.relu(self.BN4(self.Conv4(output))))\n",
        "        output = functional.relu(self.BN5(self.Conv5(output)))\n",
        "        output = self.MaxPool(functional.relu(self.BN6(self.Conv6(output))))\n",
        "        output = functional.relu(self.BN7(self.Conv7(output)))\n",
        "        output = self.MaxPool(functional.relu(self.BN8(self.Conv8(output))))\n",
        "\n",
        "        output = output.view(-1, 512)\n",
        "        output = self.dropout(functional.relu(self.FC1(output)))\n",
        "        output = self.dropout(functional.relu(self.FC2(output)))\n",
        "        output = self.FC3(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkymWB_cLME9",
        "outputId": "bf86cb23-0495-47b6-8eee-d521787a6f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./A3/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 166402686.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./A3/data/MNIST/raw/train-images-idx3-ubyte.gz to ./A3/data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./A3/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 108447353.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./A3/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./A3/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./A3/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 122171625.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./A3/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./A3/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./A3/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 24773119.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./A3/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./A3/data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resize_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = train_dataset = torchvision.datasets.MNIST(root='./A3/data',\n",
        "                                           train=True,\n",
        "                                           transform=resize_transform,\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./A3/data',\n",
        "                                          train=False,\n",
        "                                          transform=resize_transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 6000,\n",
        "                                               shuffle = True)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 10000,\n",
        "                                               shuffle = False)\n",
        "\n",
        "train_Loss_Compute_Loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 60000,\n",
        "                                               shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr83MB4ZLfqW"
      },
      "outputs": [],
      "source": [
        "# Training:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"gpu can be used\")\n",
        "num_epoch = 10\n",
        "model = VGG11().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vo8gQfoYLiC9",
        "outputId": "4a234711-ea58-41a2-963d-0b9c6719d254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "Epoch [1/10], Step [1/10], Loss: 2.3184\n",
            "Epoch [1/10], Step [2/10], Loss: 2.2900\n",
            "Epoch [1/10], Step [3/10], Loss: 2.2678\n",
            "Epoch [1/10], Step [4/10], Loss: 2.2392\n",
            "Epoch [1/10], Step [5/10], Loss: 2.2095\n",
            "Epoch [1/10], Step [6/10], Loss: 2.1905\n",
            "Epoch [1/10], Step [7/10], Loss: 2.1531\n",
            "Epoch [1/10], Step [8/10], Loss: 2.1290\n",
            "Epoch [1/10], Step [9/10], Loss: 2.1070\n",
            "Epoch [1/10], Step [10/10], Loss: 2.0666\n",
            "Epoch [2/10], Step [1/10], Loss: 2.0353\n",
            "Epoch [2/10], Step [2/10], Loss: 2.0028\n",
            "Epoch [2/10], Step [3/10], Loss: 1.9646\n",
            "Epoch [2/10], Step [4/10], Loss: 1.9268\n",
            "Epoch [2/10], Step [5/10], Loss: 1.8872\n",
            "Epoch [2/10], Step [6/10], Loss: 1.8481\n",
            "Epoch [2/10], Step [7/10], Loss: 1.8140\n",
            "Epoch [2/10], Step [8/10], Loss: 1.7671\n",
            "Epoch [2/10], Step [9/10], Loss: 1.7294\n",
            "Epoch [2/10], Step [10/10], Loss: 1.6901\n",
            "Epoch [3/10], Step [1/10], Loss: 1.6284\n",
            "Epoch [3/10], Step [2/10], Loss: 1.5912\n",
            "Epoch [3/10], Step [3/10], Loss: 1.5452\n",
            "Epoch [3/10], Step [4/10], Loss: 1.5042\n",
            "Epoch [3/10], Step [5/10], Loss: 1.4602\n",
            "Epoch [3/10], Step [6/10], Loss: 1.4165\n",
            "Epoch [3/10], Step [7/10], Loss: 1.3530\n",
            "Epoch [3/10], Step [8/10], Loss: 1.3274\n",
            "Epoch [3/10], Step [9/10], Loss: 1.2856\n",
            "Epoch [3/10], Step [10/10], Loss: 1.2353\n",
            "Epoch [4/10], Step [1/10], Loss: 1.2000\n",
            "Epoch [4/10], Step [2/10], Loss: 1.1493\n",
            "Epoch [4/10], Step [3/10], Loss: 1.1041\n",
            "Epoch [4/10], Step [4/10], Loss: 1.0728\n",
            "Epoch [4/10], Step [5/10], Loss: 1.0219\n",
            "Epoch [4/10], Step [6/10], Loss: 0.9802\n",
            "Epoch [4/10], Step [7/10], Loss: 0.9610\n",
            "Epoch [4/10], Step [8/10], Loss: 0.9153\n",
            "Epoch [4/10], Step [9/10], Loss: 0.8773\n",
            "Epoch [4/10], Step [10/10], Loss: 0.8543\n",
            "Epoch [5/10], Step [1/10], Loss: 0.8189\n",
            "Epoch [5/10], Step [2/10], Loss: 0.7951\n",
            "Epoch [5/10], Step [3/10], Loss: 0.7559\n",
            "Epoch [5/10], Step [4/10], Loss: 0.7308\n",
            "Epoch [5/10], Step [5/10], Loss: 0.7045\n",
            "Epoch [5/10], Step [6/10], Loss: 0.6809\n",
            "Epoch [5/10], Step [7/10], Loss: 0.6533\n",
            "Epoch [5/10], Step [8/10], Loss: 0.6306\n",
            "Epoch [5/10], Step [9/10], Loss: 0.6031\n",
            "Epoch [5/10], Step [10/10], Loss: 0.5759\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-39287765821d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_test_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-dda9c3b9bdf4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacty of 14.75 GiB of which 86.81 MiB is free. Process 198219 has 14.66 GiB memory in use. Of the allocated memory 2.73 GiB is allocated by PyTorch, and 10.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "print(len(train_dataloader))\n",
        "\n",
        "n_total_steps = len(train_dataloader)\n",
        "test_accuracy = []\n",
        "training_accuracy = []\n",
        "test_loss = []\n",
        "training_loss = []\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predicted_labels = model(images)\n",
        "        loss = criterion(predicted_labels, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print (f'Epoch [{epoch+1}/{num_epoch}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      total_train_loss = 0\n",
        "      total_train_correct = 0\n",
        "      for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_train_correct += (predicted_labels == labels).sum().item()\n",
        "      training_accuracy.append(total_train_correct / 60000)\n",
        "      training_loss.append(total_train_loss)\n",
        "\n",
        "      total_test_loss = 0\n",
        "      total_test_correct = 0\n",
        "      for i, (images, labels) in enumerate(test_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_test_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_test_correct += (predicted_labels == labels).sum().item()\n",
        "      test_accuracy.append(total_test_correct / 10000)\n",
        "      test_loss.append(total_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "n2puWwjuUDWi",
        "outputId": "fd637c24-0bdb-490c-968a-25c4dcf778fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fdd50b9c1a33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
          ]
        }
      ],
      "source": [
        "print(test_accuracy)\n",
        "print(training_accuracy)\n",
        "print(test_loss)\n",
        "print(training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sFIvxMBXAC9"
      },
      "outputs": [],
      "source": [
        "list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "plt.scatter(list1, test_accuracy, marker='o', color ='red')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_accuracy')\n",
        "plt.title('test_accuracy vs epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFTsaVoFYUrV"
      },
      "outputs": [],
      "source": [
        "list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "plt.scatter(list1, training_accuracy, marker='o', color ='red')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('training_accuracy')\n",
        "plt.title('training_accuracy vs epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwViUb1FYczq"
      },
      "outputs": [],
      "source": [
        "list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "plt.scatter(list1, test_loss, marker='o', color ='red')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('test_loss')\n",
        "plt.title('test_loss vs epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blFWbwCsYmSs"
      },
      "outputs": [],
      "source": [
        "list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "plt.scatter(list1, training_loss, marker='o', color ='red')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('training_loss')\n",
        "plt.title('training_loss vs epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7LXuS6Ng5Xj",
        "outputId": "62e4ba2f-9ab0-4354-95b9-7c41a087cf6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "991\n",
            "2.324692726135254\n"
          ]
        }
      ],
      "source": [
        "Horizontal_flip_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "Horizontally_flipped_test_dataset = torchvision.datasets.MNIST(root='./A3/data', train=False, transform=Horizontal_flip_transform)\n",
        "\n",
        "Horizontally_flipped_test_dataloader = torch.utils.data.DataLoader(dataset = Horizontally_flipped_test_dataset, batch_size = 10000,\n",
        "                                               shuffle = False)\n",
        "\n",
        "with torch.no_grad():\n",
        "      total_test_loss = 0\n",
        "      total_test_correct = 0\n",
        "      for i, (images, labels) in enumerate(Horizontally_flipped_test_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_test_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_test_correct += (predicted_labels == labels).sum().item()\n",
        "      print(total_test_correct)\n",
        "      print(total_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuqWdkrXkZf6",
        "outputId": "f4050d40-b5a9-46c3-907b-51fbd3756473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1049\n",
            "2.3281126022338867\n"
          ]
        }
      ],
      "source": [
        "vertical_flip_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "vertically_flipped_test_dataset = torchvision.datasets.MNIST(root='./A3/data', train=False, transform=vertical_flip_transform)\n",
        "\n",
        "vertically_flipped_test_dataloader = torch.utils.data.DataLoader(dataset = vertically_flipped_test_dataset, batch_size = 10000,\n",
        "                                               shuffle = False)\n",
        "\n",
        "with torch.no_grad():\n",
        "      total_test_loss = 0\n",
        "      total_test_correct = 0\n",
        "      for i, (images, labels) in enumerate(vertically_flipped_test_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_test_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_test_correct += (predicted_labels == labels).sum().item()\n",
        "      print(total_test_correct)\n",
        "      print(total_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tV0vH9Mk9bH",
        "outputId": "5b346214-9c8f-49f6-a061-e527af5d4b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1017\n",
            "2.3233699798583984\n"
          ]
        }
      ],
      "source": [
        "Gaussian_noise_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + 0.1*torch.randn_like(x))\n",
        "])\n",
        "\n",
        "Noised_test_dataset = torchvision.datasets.MNIST(root='./A3/data', train=False, transform=Gaussian_noise_transform)\n",
        "\n",
        "Noised_test_dataloader = torch.utils.data.DataLoader(dataset = Noised_test_dataset, batch_size = 10000,\n",
        "                                               shuffle = False)\n",
        "\n",
        "with torch.no_grad():\n",
        "      total_test_loss = 0\n",
        "      total_test_correct = 0\n",
        "      for i, (images, labels) in enumerate(Noised_test_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_test_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_test_correct += (predicted_labels == labels).sum().item()\n",
        "      print(total_test_correct)\n",
        "      print(total_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "854HPVb_oDdl",
        "outputId": "ef9e7d49-b19d-4aba-85c0-f8e5afe6f1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "Epoch [1/10], Step [1/30], Loss: 2.3383\n",
            "Epoch [1/10], Step [2/30], Loss: 2.3087\n",
            "Epoch [1/10], Step [3/30], Loss: 2.2953\n",
            "Epoch [1/10], Step [4/30], Loss: 2.2769\n",
            "Epoch [1/10], Step [5/30], Loss: 2.2677\n",
            "Epoch [1/10], Step [6/30], Loss: 2.2481\n",
            "Epoch [1/10], Step [7/30], Loss: 2.2343\n",
            "Epoch [1/10], Step [8/30], Loss: 2.2225\n",
            "Epoch [1/10], Step [9/30], Loss: 2.2001\n",
            "Epoch [1/10], Step [10/30], Loss: 2.1874\n",
            "Epoch [1/10], Step [11/30], Loss: 2.1662\n",
            "Epoch [1/10], Step [12/30], Loss: 2.1478\n",
            "Epoch [1/10], Step [13/30], Loss: 2.1332\n",
            "Epoch [1/10], Step [14/30], Loss: 2.1094\n",
            "Epoch [1/10], Step [15/30], Loss: 2.0828\n",
            "Epoch [1/10], Step [16/30], Loss: 2.0606\n",
            "Epoch [1/10], Step [17/30], Loss: 2.0385\n",
            "Epoch [1/10], Step [18/30], Loss: 2.0269\n",
            "Epoch [1/10], Step [19/30], Loss: 2.0007\n",
            "Epoch [1/10], Step [20/30], Loss: 1.9814\n",
            "Epoch [1/10], Step [21/30], Loss: 1.9466\n",
            "Epoch [1/10], Step [22/30], Loss: 1.9247\n",
            "Epoch [1/10], Step [23/30], Loss: 1.9102\n",
            "Epoch [1/10], Step [24/30], Loss: 1.8835\n",
            "Epoch [1/10], Step [25/30], Loss: 1.8625\n",
            "Epoch [1/10], Step [26/30], Loss: 1.8388\n",
            "Epoch [1/10], Step [27/30], Loss: 1.8109\n",
            "Epoch [1/10], Step [28/30], Loss: 1.7846\n",
            "Epoch [1/10], Step [29/30], Loss: 1.7665\n",
            "Epoch [1/10], Step [30/30], Loss: 1.7306\n",
            "Epoch [2/10], Step [1/30], Loss: 1.7097\n",
            "Epoch [2/10], Step [2/30], Loss: 1.6821\n",
            "Epoch [2/10], Step [3/30], Loss: 1.6445\n",
            "Epoch [2/10], Step [4/30], Loss: 1.6225\n",
            "Epoch [2/10], Step [5/30], Loss: 1.6000\n",
            "Epoch [2/10], Step [6/30], Loss: 1.5727\n",
            "Epoch [2/10], Step [7/30], Loss: 1.5554\n",
            "Epoch [2/10], Step [8/30], Loss: 1.5156\n",
            "Epoch [2/10], Step [9/30], Loss: 1.4854\n",
            "Epoch [2/10], Step [10/30], Loss: 1.4542\n",
            "Epoch [2/10], Step [11/30], Loss: 1.4248\n",
            "Epoch [2/10], Step [12/30], Loss: 1.4299\n",
            "Epoch [2/10], Step [13/30], Loss: 1.4114\n",
            "Epoch [2/10], Step [14/30], Loss: 1.3630\n",
            "Epoch [2/10], Step [15/30], Loss: 1.3400\n",
            "Epoch [2/10], Step [16/30], Loss: 1.3344\n",
            "Epoch [2/10], Step [17/30], Loss: 1.2879\n",
            "Epoch [2/10], Step [18/30], Loss: 1.2804\n",
            "Epoch [2/10], Step [19/30], Loss: 1.2510\n",
            "Epoch [2/10], Step [20/30], Loss: 1.2247\n",
            "Epoch [2/10], Step [21/30], Loss: 1.1966\n",
            "Epoch [2/10], Step [22/30], Loss: 1.1874\n",
            "Epoch [2/10], Step [23/30], Loss: 1.1618\n",
            "Epoch [2/10], Step [24/30], Loss: 1.1246\n",
            "Epoch [2/10], Step [25/30], Loss: 1.1189\n",
            "Epoch [2/10], Step [26/30], Loss: 1.0955\n",
            "Epoch [2/10], Step [27/30], Loss: 1.0889\n",
            "Epoch [2/10], Step [28/30], Loss: 1.0605\n",
            "Epoch [2/10], Step [29/30], Loss: 1.0408\n",
            "Epoch [2/10], Step [30/30], Loss: 1.0238\n",
            "Epoch [3/10], Step [1/30], Loss: 0.9955\n",
            "Epoch [3/10], Step [2/30], Loss: 0.9706\n",
            "Epoch [3/10], Step [3/30], Loss: 0.9714\n",
            "Epoch [3/10], Step [4/30], Loss: 0.9627\n",
            "Epoch [3/10], Step [5/30], Loss: 0.9219\n",
            "Epoch [3/10], Step [6/30], Loss: 0.8993\n",
            "Epoch [3/10], Step [7/30], Loss: 0.9009\n",
            "Epoch [3/10], Step [8/30], Loss: 0.8814\n",
            "Epoch [3/10], Step [9/30], Loss: 0.8647\n",
            "Epoch [3/10], Step [10/30], Loss: 0.8501\n",
            "Epoch [3/10], Step [11/30], Loss: 0.8403\n",
            "Epoch [3/10], Step [12/30], Loss: 0.8109\n",
            "Epoch [3/10], Step [13/30], Loss: 0.8100\n",
            "Epoch [3/10], Step [14/30], Loss: 0.7952\n",
            "Epoch [3/10], Step [15/30], Loss: 0.7976\n",
            "Epoch [3/10], Step [16/30], Loss: 0.7619\n",
            "Epoch [3/10], Step [17/30], Loss: 0.7581\n",
            "Epoch [3/10], Step [18/30], Loss: 0.7501\n",
            "Epoch [3/10], Step [19/30], Loss: 0.7310\n",
            "Epoch [3/10], Step [20/30], Loss: 0.7141\n",
            "Epoch [3/10], Step [21/30], Loss: 0.7033\n",
            "Epoch [3/10], Step [22/30], Loss: 0.6944\n",
            "Epoch [3/10], Step [23/30], Loss: 0.6671\n",
            "Epoch [3/10], Step [24/30], Loss: 0.6651\n",
            "Epoch [3/10], Step [25/30], Loss: 0.6529\n",
            "Epoch [3/10], Step [26/30], Loss: 0.6443\n",
            "Epoch [3/10], Step [27/30], Loss: 0.6292\n",
            "Epoch [3/10], Step [28/30], Loss: 0.6337\n",
            "Epoch [3/10], Step [29/30], Loss: 0.6281\n",
            "Epoch [3/10], Step [30/30], Loss: 0.6130\n",
            "Epoch [4/10], Step [1/30], Loss: 0.5972\n",
            "Epoch [4/10], Step [2/30], Loss: 0.5845\n",
            "Epoch [4/10], Step [3/30], Loss: 0.5784\n",
            "Epoch [4/10], Step [4/30], Loss: 0.5586\n",
            "Epoch [4/10], Step [5/30], Loss: 0.5542\n",
            "Epoch [4/10], Step [6/30], Loss: 0.5701\n",
            "Epoch [4/10], Step [7/30], Loss: 0.5454\n",
            "Epoch [4/10], Step [8/30], Loss: 0.5471\n",
            "Epoch [4/10], Step [9/30], Loss: 0.5254\n",
            "Epoch [4/10], Step [10/30], Loss: 0.5184\n",
            "Epoch [4/10], Step [11/30], Loss: 0.5218\n",
            "Epoch [4/10], Step [12/30], Loss: 0.5189\n",
            "Epoch [4/10], Step [13/30], Loss: 0.4925\n",
            "Epoch [4/10], Step [14/30], Loss: 0.4944\n",
            "Epoch [4/10], Step [15/30], Loss: 0.4981\n",
            "Epoch [4/10], Step [16/30], Loss: 0.4889\n",
            "Epoch [4/10], Step [17/30], Loss: 0.4949\n",
            "Epoch [4/10], Step [18/30], Loss: 0.4738\n",
            "Epoch [4/10], Step [19/30], Loss: 0.4668\n",
            "Epoch [4/10], Step [20/30], Loss: 0.4623\n",
            "Epoch [4/10], Step [21/30], Loss: 0.4512\n",
            "Epoch [4/10], Step [22/30], Loss: 0.4464\n",
            "Epoch [4/10], Step [23/30], Loss: 0.4417\n",
            "Epoch [4/10], Step [24/30], Loss: 0.4558\n",
            "Epoch [4/10], Step [25/30], Loss: 0.4266\n",
            "Epoch [4/10], Step [26/30], Loss: 0.4315\n",
            "Epoch [4/10], Step [27/30], Loss: 0.4290\n",
            "Epoch [4/10], Step [28/30], Loss: 0.4093\n",
            "Epoch [4/10], Step [29/30], Loss: 0.4181\n",
            "Epoch [4/10], Step [30/30], Loss: 0.4041\n",
            "Epoch [5/10], Step [1/30], Loss: 0.3955\n",
            "Epoch [5/10], Step [2/30], Loss: 0.3954\n",
            "Epoch [5/10], Step [3/30], Loss: 0.3910\n",
            "Epoch [5/10], Step [4/30], Loss: 0.3816\n",
            "Epoch [5/10], Step [5/30], Loss: 0.3823\n",
            "Epoch [5/10], Step [6/30], Loss: 0.3736\n",
            "Epoch [5/10], Step [7/30], Loss: 0.3664\n",
            "Epoch [5/10], Step [8/30], Loss: 0.3823\n",
            "Epoch [5/10], Step [9/30], Loss: 0.3697\n",
            "Epoch [5/10], Step [10/30], Loss: 0.3648\n",
            "Epoch [5/10], Step [11/30], Loss: 0.3651\n",
            "Epoch [5/10], Step [12/30], Loss: 0.3527\n",
            "Epoch [5/10], Step [13/30], Loss: 0.3529\n",
            "Epoch [5/10], Step [14/30], Loss: 0.3500\n",
            "Epoch [5/10], Step [15/30], Loss: 0.3554\n",
            "Epoch [5/10], Step [16/30], Loss: 0.3397\n",
            "Epoch [5/10], Step [17/30], Loss: 0.3379\n",
            "Epoch [5/10], Step [18/30], Loss: 0.3256\n",
            "Epoch [5/10], Step [19/30], Loss: 0.3404\n",
            "Epoch [5/10], Step [20/30], Loss: 0.3264\n",
            "Epoch [5/10], Step [21/30], Loss: 0.3271\n",
            "Epoch [5/10], Step [22/30], Loss: 0.3121\n",
            "Epoch [5/10], Step [23/30], Loss: 0.3104\n",
            "Epoch [5/10], Step [24/30], Loss: 0.3247\n",
            "Epoch [5/10], Step [25/30], Loss: 0.3141\n",
            "Epoch [5/10], Step [26/30], Loss: 0.3129\n",
            "Epoch [5/10], Step [27/30], Loss: 0.3108\n",
            "Epoch [5/10], Step [28/30], Loss: 0.3088\n",
            "Epoch [5/10], Step [29/30], Loss: 0.3022\n",
            "Epoch [5/10], Step [30/30], Loss: 0.3056\n",
            "Epoch [6/10], Step [1/30], Loss: 0.2876\n",
            "Epoch [6/10], Step [2/30], Loss: 0.2886\n",
            "Epoch [6/10], Step [3/30], Loss: 0.2881\n",
            "Epoch [6/10], Step [4/30], Loss: 0.2895\n",
            "Epoch [6/10], Step [5/30], Loss: 0.2912\n",
            "Epoch [6/10], Step [6/30], Loss: 0.2930\n",
            "Epoch [6/10], Step [7/30], Loss: 0.2773\n",
            "Epoch [6/10], Step [8/30], Loss: 0.2713\n",
            "Epoch [6/10], Step [9/30], Loss: 0.2753\n",
            "Epoch [6/10], Step [10/30], Loss: 0.2736\n",
            "Epoch [6/10], Step [11/30], Loss: 0.2613\n",
            "Epoch [6/10], Step [12/30], Loss: 0.2799\n",
            "Epoch [6/10], Step [13/30], Loss: 0.2716\n",
            "Epoch [6/10], Step [14/30], Loss: 0.2712\n",
            "Epoch [6/10], Step [15/30], Loss: 0.2554\n",
            "Epoch [6/10], Step [16/30], Loss: 0.2576\n",
            "Epoch [6/10], Step [17/30], Loss: 0.2655\n",
            "Epoch [6/10], Step [18/30], Loss: 0.2537\n",
            "Epoch [6/10], Step [19/30], Loss: 0.2412\n",
            "Epoch [6/10], Step [20/30], Loss: 0.2596\n",
            "Epoch [6/10], Step [21/30], Loss: 0.2490\n",
            "Epoch [6/10], Step [22/30], Loss: 0.2476\n",
            "Epoch [6/10], Step [23/30], Loss: 0.2420\n",
            "Epoch [6/10], Step [24/30], Loss: 0.2427\n",
            "Epoch [6/10], Step [25/30], Loss: 0.2468\n",
            "Epoch [6/10], Step [26/30], Loss: 0.2427\n",
            "Epoch [6/10], Step [27/30], Loss: 0.2264\n",
            "Epoch [6/10], Step [28/30], Loss: 0.2369\n",
            "Epoch [6/10], Step [29/30], Loss: 0.2420\n",
            "Epoch [6/10], Step [30/30], Loss: 0.2347\n",
            "Epoch [7/10], Step [1/30], Loss: 0.2346\n",
            "Epoch [7/10], Step [2/30], Loss: 0.2230\n",
            "Epoch [7/10], Step [3/30], Loss: 0.2318\n",
            "Epoch [7/10], Step [4/30], Loss: 0.2301\n",
            "Epoch [7/10], Step [5/30], Loss: 0.2282\n",
            "Epoch [7/10], Step [6/30], Loss: 0.2362\n",
            "Epoch [7/10], Step [7/30], Loss: 0.2314\n",
            "Epoch [7/10], Step [8/30], Loss: 0.2153\n",
            "Epoch [7/10], Step [9/30], Loss: 0.2157\n",
            "Epoch [7/10], Step [10/30], Loss: 0.2206\n",
            "Epoch [7/10], Step [11/30], Loss: 0.2175\n",
            "Epoch [7/10], Step [12/30], Loss: 0.2105\n",
            "Epoch [7/10], Step [13/30], Loss: 0.2110\n",
            "Epoch [7/10], Step [14/30], Loss: 0.2134\n",
            "Epoch [7/10], Step [15/30], Loss: 0.2082\n",
            "Epoch [7/10], Step [16/30], Loss: 0.2114\n",
            "Epoch [7/10], Step [17/30], Loss: 0.2060\n",
            "Epoch [7/10], Step [18/30], Loss: 0.2070\n",
            "Epoch [7/10], Step [19/30], Loss: 0.1992\n",
            "Epoch [7/10], Step [20/30], Loss: 0.2129\n",
            "Epoch [7/10], Step [21/30], Loss: 0.2172\n",
            "Epoch [7/10], Step [22/30], Loss: 0.2162\n",
            "Epoch [7/10], Step [23/30], Loss: 0.1934\n",
            "Epoch [7/10], Step [24/30], Loss: 0.1873\n",
            "Epoch [7/10], Step [25/30], Loss: 0.2035\n",
            "Epoch [7/10], Step [26/30], Loss: 0.1931\n",
            "Epoch [7/10], Step [27/30], Loss: 0.2003\n",
            "Epoch [7/10], Step [28/30], Loss: 0.1919\n",
            "Epoch [7/10], Step [29/30], Loss: 0.1901\n",
            "Epoch [7/10], Step [30/30], Loss: 0.1920\n",
            "Epoch [8/10], Step [1/30], Loss: 0.1844\n",
            "Epoch [8/10], Step [2/30], Loss: 0.1931\n",
            "Epoch [8/10], Step [3/30], Loss: 0.1983\n",
            "Epoch [8/10], Step [4/30], Loss: 0.1916\n",
            "Epoch [8/10], Step [5/30], Loss: 0.1843\n",
            "Epoch [8/10], Step [6/30], Loss: 0.1833\n",
            "Epoch [8/10], Step [7/30], Loss: 0.1978\n",
            "Epoch [8/10], Step [8/30], Loss: 0.1754\n",
            "Epoch [8/10], Step [9/30], Loss: 0.1764\n",
            "Epoch [8/10], Step [10/30], Loss: 0.1862\n",
            "Epoch [8/10], Step [11/30], Loss: 0.1769\n",
            "Epoch [8/10], Step [12/30], Loss: 0.1777\n",
            "Epoch [8/10], Step [13/30], Loss: 0.1699\n",
            "Epoch [8/10], Step [14/30], Loss: 0.1784\n",
            "Epoch [8/10], Step [15/30], Loss: 0.1819\n",
            "Epoch [8/10], Step [16/30], Loss: 0.1761\n",
            "Epoch [8/10], Step [17/30], Loss: 0.1842\n",
            "Epoch [8/10], Step [18/30], Loss: 0.1839\n",
            "Epoch [8/10], Step [19/30], Loss: 0.1624\n",
            "Epoch [8/10], Step [20/30], Loss: 0.1688\n",
            "Epoch [8/10], Step [21/30], Loss: 0.1705\n",
            "Epoch [8/10], Step [22/30], Loss: 0.1652\n",
            "Epoch [8/10], Step [23/30], Loss: 0.1695\n",
            "Epoch [8/10], Step [24/30], Loss: 0.1686\n",
            "Epoch [8/10], Step [25/30], Loss: 0.1591\n",
            "Epoch [8/10], Step [26/30], Loss: 0.1737\n",
            "Epoch [8/10], Step [27/30], Loss: 0.1622\n",
            "Epoch [8/10], Step [28/30], Loss: 0.1650\n",
            "Epoch [8/10], Step [29/30], Loss: 0.1731\n",
            "Epoch [8/10], Step [30/30], Loss: 0.1643\n",
            "Epoch [9/10], Step [1/30], Loss: 0.1457\n",
            "Epoch [9/10], Step [2/30], Loss: 0.1611\n",
            "Epoch [9/10], Step [3/30], Loss: 0.1566\n",
            "Epoch [9/10], Step [4/30], Loss: 0.1675\n",
            "Epoch [9/10], Step [5/30], Loss: 0.1578\n",
            "Epoch [9/10], Step [6/30], Loss: 0.1557\n",
            "Epoch [9/10], Step [7/30], Loss: 0.1634\n",
            "Epoch [9/10], Step [8/30], Loss: 0.1650\n",
            "Epoch [9/10], Step [9/30], Loss: 0.1652\n",
            "Epoch [9/10], Step [10/30], Loss: 0.1436\n",
            "Epoch [9/10], Step [11/30], Loss: 0.1499\n",
            "Epoch [9/10], Step [12/30], Loss: 0.1621\n",
            "Epoch [9/10], Step [13/30], Loss: 0.1522\n",
            "Epoch [9/10], Step [14/30], Loss: 0.1530\n",
            "Epoch [9/10], Step [15/30], Loss: 0.1645\n",
            "Epoch [9/10], Step [16/30], Loss: 0.1620\n",
            "Epoch [9/10], Step [17/30], Loss: 0.1458\n",
            "Epoch [9/10], Step [18/30], Loss: 0.1439\n",
            "Epoch [9/10], Step [19/30], Loss: 0.1526\n",
            "Epoch [9/10], Step [20/30], Loss: 0.1511\n",
            "Epoch [9/10], Step [21/30], Loss: 0.1510\n",
            "Epoch [9/10], Step [22/30], Loss: 0.1475\n",
            "Epoch [9/10], Step [23/30], Loss: 0.1494\n",
            "Epoch [9/10], Step [24/30], Loss: 0.1421\n",
            "Epoch [9/10], Step [25/30], Loss: 0.1449\n",
            "Epoch [9/10], Step [26/30], Loss: 0.1436\n",
            "Epoch [9/10], Step [27/30], Loss: 0.1489\n",
            "Epoch [9/10], Step [28/30], Loss: 0.1438\n",
            "Epoch [9/10], Step [29/30], Loss: 0.1426\n",
            "Epoch [9/10], Step [30/30], Loss: 0.1414\n",
            "Epoch [10/10], Step [1/30], Loss: 0.1413\n",
            "Epoch [10/10], Step [2/30], Loss: 0.1418\n",
            "Epoch [10/10], Step [3/30], Loss: 0.1368\n",
            "Epoch [10/10], Step [4/30], Loss: 0.1299\n",
            "Epoch [10/10], Step [5/30], Loss: 0.1409\n",
            "Epoch [10/10], Step [6/30], Loss: 0.1389\n",
            "Epoch [10/10], Step [7/30], Loss: 0.1339\n",
            "Epoch [10/10], Step [8/30], Loss: 0.1330\n",
            "Epoch [10/10], Step [9/30], Loss: 0.1343\n",
            "Epoch [10/10], Step [10/30], Loss: 0.1444\n",
            "Epoch [10/10], Step [11/30], Loss: 0.1402\n",
            "Epoch [10/10], Step [12/30], Loss: 0.1292\n",
            "Epoch [10/10], Step [13/30], Loss: 0.1380\n",
            "Epoch [10/10], Step [14/30], Loss: 0.1318\n",
            "Epoch [10/10], Step [15/30], Loss: 0.1464\n",
            "Epoch [10/10], Step [16/30], Loss: 0.1424\n",
            "Epoch [10/10], Step [17/30], Loss: 0.1224\n",
            "Epoch [10/10], Step [18/30], Loss: 0.1376\n",
            "Epoch [10/10], Step [19/30], Loss: 0.1278\n",
            "Epoch [10/10], Step [20/30], Loss: 0.1201\n",
            "Epoch [10/10], Step [21/30], Loss: 0.1353\n",
            "Epoch [10/10], Step [22/30], Loss: 0.1377\n",
            "Epoch [10/10], Step [23/30], Loss: 0.1379\n",
            "Epoch [10/10], Step [24/30], Loss: 0.1230\n",
            "Epoch [10/10], Step [25/30], Loss: 0.1216\n",
            "Epoch [10/10], Step [26/30], Loss: 0.1278\n",
            "Epoch [10/10], Step [27/30], Loss: 0.1361\n",
            "Epoch [10/10], Step [28/30], Loss: 0.1301\n",
            "Epoch [10/10], Step [29/30], Loss: 0.1318\n",
            "Epoch [10/10], Step [30/30], Loss: 0.1210\n"
          ]
        }
      ],
      "source": [
        "train_dataset = train_dataset = torchvision.datasets.MNIST(root='./A3/data',\n",
        "                                           train=True,\n",
        "                                           transform=resize_transform,\n",
        "                                           download=False)\n",
        "horizontal_flip_train_dataset = torchvision.datasets.MNIST(root='./A3/data',\n",
        "                                           train=True,\n",
        "                                           transform=Horizontal_flip_transform,\n",
        "                                           download=False)\n",
        "\n",
        "vertical_flip_train_dataset = torchvision.datasets.MNIST(root='./A3/data',\n",
        "                                           train=True,\n",
        "                                           transform=vertical_flip_transform,\n",
        "                                           download=False)\n",
        "\n",
        "concatenated_dataset = torch.utils.data.ConcatDataset([train_dataset,\n",
        "                                                       horizontal_flip_train_dataset,\n",
        "                                                       vertical_flip_train_dataset])\n",
        "\n",
        "new_train_dataloader = torch.utils.data.DataLoader(dataset = concatenated_dataset, batch_size = 6000,\n",
        "                                               shuffle = True)\n",
        "\n",
        "\n",
        "\n",
        "model_augmentation = VGG11().to(device)\n",
        "optimizer_augmentation = torch.optim.SGD(model_augmentation.parameters(), lr=0.01)\n",
        "\n",
        "print(len(new_train_dataloader))\n",
        "\n",
        "n_total_steps = len(new_train_dataloader)\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (images, labels) in enumerate(new_train_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        predicted_labels = model_augmentation(images)\n",
        "        loss = criterion(predicted_labels, labels)\n",
        "\n",
        "        optimizer_augmentation.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_augmentation.step()\n",
        "\n",
        "        print (f'Epoch [{epoch+1}/{num_epoch}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BohbNEtRu8L0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7ce915-9508-49db-eade-8cb24c3de1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28734\n",
            "14.555351978167892\n"
          ]
        }
      ],
      "source": [
        "concatenated_test_dataset = torch.utils.data.ConcatDataset([test_dataset,\n",
        "                                                              Horizontally_flipped_test_dataset,\n",
        "                                                              vertically_flipped_test_dataset])\n",
        "new_test_dataloader = torch.utils.data.DataLoader(dataset = concatenated_test_dataset, batch_size = 300,\n",
        "                                               shuffle = False)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "      total_test_loss = 0\n",
        "      total_test_correct = 0\n",
        "      for i, (images, labels) in enumerate(new_test_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model_augmentation(images)\n",
        "        loss = criterion(output, labels)\n",
        "        total_test_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_test_correct += (predicted_labels == labels).sum().item()\n",
        "      print(total_test_correct)\n",
        "      print(total_test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}